# Milvus ä¸­çš„ Metadata æ”¯æŒ

## æ¦‚è¿°

ç³»ç»Ÿç°åœ¨åœ¨ Milvus ä¸­å­˜å‚¨**ä¸°å¯Œçš„ metadata**ï¼ˆå…ƒæ•°æ®ï¼‰ï¼Œæ£€ç´¢æ—¶å¯ä»¥è·å–è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯ï¼Œä¸ä»…ä»…æ˜¯æ–‡æœ¬å†…å®¹ã€‚

## Metadata å­—æ®µ

### å®Œæ•´å­—æ®µåˆ—è¡¨

```python
{
    # ç³»ç»Ÿå­—æ®µ
    "id": 123456,                    # Milvusä¸»é”®ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
    "distance": 0.234,               # å‘é‡è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰
    "relevance_score": 0.810,        # ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šç›¸å…³ï¼‰
    
    # Paper åŸºæœ¬ä¿¡æ¯
    "paper_id": "20260120_paper.pdf",      # MinIOå¯¹è±¡åï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰
    "title": "Deep Learning for NLP",      # è®ºæ–‡æ ‡é¢˜
    "file_name": "paper.pdf",              # åŸå§‹æ–‡ä»¶å
    "upload_time": "2026-01-20T...",       # ä¸Šä¼ æ—¶é—´ï¼ˆISO 8601ï¼‰
    
    # Chunk ä¿¡æ¯
    "chunk_id": "20260120_paper.pdf#chunk_0",  # Chunkå”¯ä¸€ID
    "chunk_index": 0,                          # Chunkç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
    "content": "Abstract: This paper...",      # Chunkæ–‡æœ¬å†…å®¹
    "chunk_chars": 987,                        # Chunkå­—ç¬¦æ•°
    "page_range": "1-3",                       # é¡µç èŒƒå›´ï¼ˆä¼°ç®—ï¼‰
    "source": "chunk"                          # æ¥æºç±»å‹
}
```

## å­—æ®µè¯¦è§£

### 1. Paper åŸºæœ¬ä¿¡æ¯

#### `paper_id` (VARCHAR 255)
- **å«ä¹‰**: è®ºæ–‡çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆMinIOå¯¹è±¡åï¼‰
- **æ ¼å¼**: `YYYYMMDD_HHMMSS_filename.pdf`
- **ç”¨é€”**: 
  - å…³è”åŒä¸€ç¯‡è®ºæ–‡çš„æ‰€æœ‰chunks
  - åˆ é™¤è®ºæ–‡æ—¶çº§è”åˆ é™¤æ‰€æœ‰chunks
  - åœ¨MinIOä¸­å®šä½åŸå§‹PDFæ–‡ä»¶

#### `title` (VARCHAR 1000)
- **å«ä¹‰**: è®ºæ–‡æ ‡é¢˜
- **æ¥æº**: ä»æ–‡ä»¶åæå–ï¼ˆå»é™¤`.pdf`å’Œ`_`ï¼‰
- **ç”¨é€”**: 
  - æ˜¾ç¤ºç»™ç”¨æˆ·
  - æŒ‰è®ºæ–‡åˆ†ç»„å±•ç¤º
  - æœç´¢ç»“æœæ ‡é¢˜

#### `file_name` (VARCHAR 500)
- **å«ä¹‰**: ç”¨æˆ·ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶å
- **æ ¼å¼**: `paper.pdf`
- **ç”¨é€”**: 
  - æ˜¾ç¤ºç”¨æˆ·å‹å¥½çš„æ–‡ä»¶å
  - ä¸‹è½½æ—¶ä½¿ç”¨
  - æœç´¢å’Œè¿‡æ»¤

#### `upload_time` (VARCHAR 50)
- **å«ä¹‰**: è®ºæ–‡ä¸Šä¼ æ—¶é—´
- **æ ¼å¼**: ISO 8601 (`2026-01-20T12:34:56.789Z`)
- **ç”¨é€”**: 
  - æŒ‰æ—¶é—´æ’åº
  - æ˜¾ç¤º"æœ€è¿‘ä¸Šä¼ "
  - æ—¶é—´èŒƒå›´è¿‡æ»¤

### 2. Chunk ä¿¡æ¯

#### `chunk_id` (VARCHAR 300)
- **å«ä¹‰**: Chunkçš„å…¨å±€å”¯ä¸€æ ‡è¯†
- **æ ¼å¼**: `{paper_id}#chunk_{index}`
- **ç”¨é€”**: 
  - ç²¾ç¡®å®šä½åˆ°æŸä¸ªchunk
  - å¼•ç”¨æº¯æº
  - å»é‡

#### `chunk_index` (INT64)
- **å«ä¹‰**: Chunkåœ¨è®ºæ–‡ä¸­çš„åºå·ï¼ˆä»0å¼€å§‹ï¼‰
- **ç”¨é€”**: 
  - æ’åºï¼ˆæŒ‰æ–‡æ¡£é¡ºåºï¼‰
  - é‡ç»„ä¸Šä¸‹æ–‡
  - æ˜¾ç¤º"ç¬¬Næ®µ"

#### `content` (VARCHAR 65535)
- **å«ä¹‰**: Chunkçš„æ–‡æœ¬å†…å®¹
- **å¤§å°**: æœ€å¤š65535å­—ç¬¦ï¼ˆçº¦16K tokensï¼‰
- **ç”¨é€”**: 
  - æ˜¾ç¤ºæœç´¢ç»“æœ
  - RAGä¸Šä¸‹æ–‡
  - æ–‡æœ¬åˆ†æ

#### `chunk_chars` (INT64)
- **å«ä¹‰**: Chunkçš„å®é™…å­—ç¬¦æ•°
- **èŒƒå›´**: é€šå¸¸ 800-1200ï¼ˆç›®æ ‡1000ï¼‰
- **ç”¨é€”**: 
  - è´¨é‡æ£€æŸ¥
  - ç»Ÿè®¡åˆ†æ
  - è®¡è´¹ä¼°ç®—

#### `page_range` (VARCHAR 50)
- **å«ä¹‰**: Chunkå¯¹åº”çš„é¡µç èŒƒå›´ï¼ˆä¼°ç®—ï¼‰
- **æ ¼å¼**: `"1-3"` æˆ– `"5-7"`
- **ç”¨é€”**: 
  - æ˜¾ç¤ºæ¥æºä½ç½®
  - å¿«é€Ÿå®šä½
  - PDFå¯¼èˆª

#### `source` (VARCHAR 100)
- **å«ä¹‰**: Chunkçš„æ¥æºç±»å‹
- **å€¼**: `"chunk"` (å½“å‰éƒ½æ˜¯chunkç±»å‹)
- **ç”¨é€”**: 
  - åŒºåˆ†ä¸åŒç±»å‹çš„æ–‡æœ¬
  - æœªæ¥æ‰©å±•ï¼ˆå¦‚"abstract", "figure_caption"ç­‰ï¼‰

## æ£€ç´¢ç¤ºä¾‹

### 1. åŸºç¡€æ£€ç´¢ï¼ˆè·å–æ‰€æœ‰metadataï¼‰

```python
from app.services.milvus_service import MilvusService
from app.services.openai_service import OpenAIService

# åˆå§‹åŒ–
milvus = MilvusService()
openai = OpenAIService()
milvus.connect()

# ç”ŸæˆæŸ¥è¯¢å‘é‡
query = "What is transformer architecture?"
query_embedding = await openai.generate_embeddings([query])

# æ£€ç´¢ï¼ˆè‡ªåŠ¨è¿”å›æ‰€æœ‰metadataï¼‰
results = milvus.search_similar(
    query_vectors=query_embedding,
    top_k=5
)

# æŸ¥çœ‹ç»“æœ
for hit in results[0]:
    print(f"ğŸ“„ {hit['file_name']}")
    print(f"   æ ‡é¢˜: {hit['title']}")
    print(f"   Chunk: #{hit['chunk_index']} (é¡µç : {hit['page_range']})")
    print(f"   ä¸Šä¼ : {hit['upload_time']}")
    print(f"   ç›¸å…³æ€§: {hit['relevance_score']:.2%}")
    print(f"   å†…å®¹: {hit['content'][:200]}...")
    print()
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
ğŸ“„ transformer_paper.pdf
   æ ‡é¢˜: Attention Is All You Need
   Chunk: #3 (é¡µç : 2-4)
   ä¸Šä¼ : 2026-01-20T14:32:17.123Z
   ç›¸å…³æ€§: 91.25%
   å†…å®¹: The Transformer model architecture is based entirely on 
         attention mechanisms, dispensing with recurrence and 
         convolutions entirely...
```

### 2. æŒ‰è®ºæ–‡åˆ†ç»„å±•ç¤º

```python
# æ£€ç´¢åæŒ‰paper_idåˆ†ç»„
results = milvus.search_similar(query_vectors=query_embedding, top_k=20)

papers = {}
for hit in results[0]:
    paper_id = hit['paper_id']
    if paper_id not in papers:
        papers[paper_id] = {
            'file_name': hit['file_name'],
            'title': hit['title'],
            'upload_time': hit['upload_time'],
            'chunks': []
        }
    papers[paper_id]['chunks'].append({
        'chunk_index': hit['chunk_index'],
        'page_range': hit['page_range'],
        'content': hit['content'],
        'relevance': hit['relevance_score']
    })

# æ˜¾ç¤º
for paper_id, paper in papers.items():
    print(f"\nğŸ“š {paper['file_name']}")
    print(f"   {paper['title']}")
    print(f"   ç›¸å…³ç‰‡æ®µ ({len(paper['chunks'])}ä¸ª):")
    for chunk in paper['chunks'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"     - Chunk #{chunk['chunk_index']} (é¡µç : {chunk['page_range']})")
        print(f"       ç›¸å…³æ€§: {chunk['relevance']:.2%}")
```

### 3. æ—¶é—´èŒƒå›´è¿‡æ»¤

```python
from datetime import datetime, timedelta

# è·å–æœ€è¿‘7å¤©çš„è®ºæ–‡
seven_days_ago = (datetime.utcnow() - timedelta(days=7)).isoformat() + 'Z'

# æ£€ç´¢å¹¶è¿‡æ»¤
results = milvus.search_similar(query_vectors=query_embedding, top_k=50)

recent_results = [
    hit for hit in results[0]
    if hit['upload_time'] >= seven_days_ago
]

print(f"æœ€è¿‘7å¤©ä¸Šä¼ çš„ç›¸å…³è®ºæ–‡: {len(recent_results)}ç¯‡")
```

### 4. æ„å»º RAG Contextï¼ˆå¸¦metadataï¼‰

```python
async def build_rag_context(chunks: List[Dict]) -> str:
    """æ„å»ºRAGä¸Šä¸‹æ–‡ï¼ˆåŒ…å«metadataï¼‰"""
    
    context_parts = []
    
    for i, chunk in enumerate(chunks):
        # åŒ…å«æ¥æºä¿¡æ¯
        source_info = (
            f"[æ¥æº {i+1}] {chunk['file_name']}\n"
            f"æ ‡é¢˜: {chunk['title']}\n"
            f"ä½ç½®: ç¬¬ {chunk['chunk_index']+1} æ®µ (é¡µç : {chunk['page_range']})\n"
            f"ä¸Šä¼ æ—¶é—´: {chunk['upload_time'][:10]}\n"
            f"ç›¸å…³æ€§: {chunk['relevance_score']:.1%}\n"
            f"\n{chunk['content']}\n"
        )
        context_parts.append(source_info)
    
    return "\n" + "="*60 + "\n\n".join(context_parts)

# ä½¿ç”¨
chunks = results[0][:5]
context = await build_rag_context(chunks)

prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒæ–‡çŒ®å›ç­”é—®é¢˜ã€‚

{context}

é—®é¢˜: {query}

è¯·æä¾›è¯¦ç»†å›ç­”ï¼Œå¹¶æ³¨æ˜ä¿¡æ¯æ¥æºï¼ˆä½¿ç”¨ [æ¥æº N] å¼•ç”¨ï¼‰ã€‚
"""
```

## é«˜çº§ç”¨æ³•

### 1. å¼•ç”¨æº¯æº

```python
def generate_citation(chunk: Dict) -> str:
    """ç”Ÿæˆå¼•ç”¨æ ¼å¼"""
    return (
        f"{chunk['file_name']} "
        f"(ä¸Šä¼ äº {chunk['upload_time'][:10]}), "
        f"ç¬¬ {chunk['chunk_index']+1} æ®µ, "
        f"é¡µç  {chunk['page_range']}"
    )

# ä½¿ç”¨
for hit in results[0][:3]:
    print(f"å†…å®¹: {hit['content'][:100]}...")
    print(f"å¼•ç”¨: {generate_citation(hit)}")
```

### 2. å»é‡ï¼ˆé¿å…åŒä¸€è®ºæ–‡å¤šä¸ªchunksï¼‰

```python
def deduplicate_by_paper(results: List[Dict], max_per_paper: int = 2) -> List[Dict]:
    """æ¯ç¯‡è®ºæ–‡æœ€å¤šä¿ç•™Nä¸ªchunks"""
    
    paper_counts = {}
    deduplicated = []
    
    for hit in results:
        paper_id = hit['paper_id']
        count = paper_counts.get(paper_id, 0)
        
        if count < max_per_paper:
            deduplicated.append(hit)
            paper_counts[paper_id] = count + 1
    
    return deduplicated

# ä½¿ç”¨
results = milvus.search_similar(query_vectors=query_embedding, top_k=20)
unique_results = deduplicate_by_paper(results[0], max_per_paper=2)
```

### 3. æ™ºèƒ½æ’åº

```python
def smart_sort(chunks: List[Dict]) -> List[Dict]:
    """
    æ™ºèƒ½æ’åºï¼š
    1. æŒ‰ç›¸å…³æ€§åˆ†ç»„
    2. åŒä¸€è®ºæ–‡çš„chunksæŒ‰chunk_indexæ’åº
    """
    
    # æŒ‰paper_idåˆ†ç»„
    papers = {}
    for chunk in chunks:
        pid = chunk['paper_id']
        if pid not in papers:
            papers[pid] = []
        papers[pid].append(chunk)
    
    # æ¯ä¸ªè®ºæ–‡å†…éƒ¨æŒ‰chunk_indexæ’åº
    for pid in papers:
        papers[pid].sort(key=lambda x: x['chunk_index'])
    
    # æŒ‰ç¬¬ä¸€ä¸ªchunkçš„ç›¸å…³æ€§æ’åºè®ºæ–‡
    sorted_papers = sorted(
        papers.items(),
        key=lambda x: x[1][0]['relevance_score'],
        reverse=True
    )
    
    # å±•å¹³
    result = []
    for pid, paper_chunks in sorted_papers:
        result.extend(paper_chunks)
    
    return result
```

## API é›†æˆç¤ºä¾‹

### åˆ›å»ºæœç´¢ API

```python
from fastapi import APIRouter, Query
from typing import List, Optional

router = APIRouter(prefix="/api/papers", tags=["papers"])

@router.post("/search")
async def semantic_search(
    query: str,
    top_k: int = Query(10, ge=1, le=50),
    uploaded_after: Optional[str] = None
):
    """
    è¯­ä¹‰æœç´¢APIï¼ˆè¿”å›å®Œæ•´metadataï¼‰
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        top_k: è¿”å›ç»“æœæ•°é‡
        uploaded_after: è¿‡æ»¤ä¸Šä¼ æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
    
    Returns:
        {
            "query": "...",
            "results": [
                {
                    "paper_id": "...",
                    "file_name": "...",
                    "title": "...",
                    "chunk_index": 0,
                    "page_range": "1-3",
                    "upload_time": "...",
                    "relevance_score": 0.95,
                    "content": "...",
                    ...
                }
            ],
            "total": 10
        }
    """
    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    openai_service = get_openai_service()
    query_embedding = await openai_service.generate_embeddings([query])
    
    # æ£€ç´¢
    milvus_service = get_milvus_service()
    results = milvus_service.search_similar(
        query_vectors=query_embedding,
        top_k=top_k
    )
    
    # è¿‡æ»¤ï¼ˆå¦‚æœæœ‰æ—¶é—´é™åˆ¶ï¼‰
    chunks = results[0]
    if uploaded_after:
        chunks = [c for c in chunks if c['upload_time'] >= uploaded_after]
    
    return {
        "query": query,
        "results": chunks,
        "total": len(chunks)
    }
```

## å‰ç«¯å±•ç¤ºç¤ºä¾‹

### Vue ç»„ä»¶

```vue
<template>
  <div class="search-results">
    <div v-for="result in results" :key="result.chunk_id" class="result-card">
      <!-- Paperä¿¡æ¯ -->
      <div class="paper-info">
        <h3>{{ result.title }}</h3>
        <div class="metadata">
          <span class="file-name">ğŸ“„ {{ result.file_name }}</span>
          <span class="upload-time">ğŸ•’ {{ formatDate(result.upload_time) }}</span>
          <span class="relevance">â­ {{ (result.relevance_score * 100).toFixed(1) }}%</span>
        </div>
      </div>
      
      <!-- Chunkä¿¡æ¯ -->
      <div class="chunk-info">
        <span class="chunk-badge">
          Chunk #{{ result.chunk_index + 1 }} 
          (é¡µç : {{ result.page_range }})
        </span>
      </div>
      
      <!-- å†…å®¹ -->
      <div class="content">
        {{ result.content }}
      </div>
      
      <!-- æ“ä½œ -->
      <div class="actions">
        <button @click="viewPaper(result.paper_id)">æŸ¥çœ‹å…¨æ–‡</button>
        <button @click="copyReference(result)">å¤åˆ¶å¼•ç”¨</button>
      </div>
    </div>
  </div>
</template>

<script>
export default {
  methods: {
    formatDate(isoString) {
      return new Date(isoString).toLocaleDateString('zh-CN')
    },
    
    copyReference(result) {
      const ref = `${result.file_name}, ç¬¬${result.chunk_index + 1}æ®µ (é¡µç : ${result.page_range})`
      navigator.clipboard.writeText(ref)
    }
  }
}
</script>
```

## æ€§èƒ½ä¼˜åŒ–

### 1. åªè¿”å›éœ€è¦çš„å­—æ®µ

```python
# å¦‚æœåªéœ€è¦éƒ¨åˆ†å­—æ®µ
results = milvus_service.collection.search(
    data=query_vectors,
    anns_field="embedding",
    limit=10,
    output_fields=["paper_id", "title", "content"]  # åªè¿”å›è¿™3ä¸ªå­—æ®µ
)
```

### 2. ç¼“å­˜metadata

```python
# ç¼“å­˜è®ºæ–‡çš„åŸºæœ¬ä¿¡æ¯
paper_cache = {}

def get_paper_metadata(paper_id: str) -> Dict:
    if paper_id not in paper_cache:
        # ä»MilvusæŸ¥è¯¢è¯¥è®ºæ–‡çš„ä»»æ„ä¸€ä¸ªchunkè·å–metadata
        results = milvus.collection.query(
            expr=f'paper_id == "{paper_id}"',
            output_fields=["title", "file_name", "upload_time"],
            limit=1
        )
        paper_cache[paper_id] = results[0]
    
    return paper_cache[paper_id]
```

## æ€»ç»“

é€šè¿‡ä¸°å¯Œçš„ metadataï¼Œç³»ç»Ÿç°åœ¨å¯ä»¥ï¼š

âœ… **ç²¾ç¡®æº¯æº** - çŸ¥é“å†…å®¹æ¥è‡ªå“ªç¯‡è®ºæ–‡çš„å“ªä¸€é¡µ  
âœ… **æ™ºèƒ½æ’åº** - æŒ‰ç›¸å…³æ€§ã€æ—¶é—´ç­‰å¤šç»´åº¦æ’åº  
âœ… **ç”¨æˆ·å‹å¥½** - æ˜¾ç¤ºåŸå§‹æ–‡ä»¶åå’Œä¸Šä¼ æ—¶é—´  
âœ… **å¼•ç”¨æ”¯æŒ** - è‡ªåŠ¨ç”Ÿæˆå­¦æœ¯å¼•ç”¨æ ¼å¼  
âœ… **è¿‡æ»¤ç­›é€‰** - æŒ‰æ—¶é—´ã€æ¥æºç­‰æ¡ä»¶è¿‡æ»¤  
âœ… **RAGå¢å¼º** - æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯  

è¿™äº› metadata è®©æ£€ç´¢ç»“æœæ›´åŠ å®ç”¨å’Œä¸“ä¸šï¼ğŸ¯

