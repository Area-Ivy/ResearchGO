"""
LangGraph Agent Implementation

é›†æˆä¸‰å±‚è®°å¿†ç³»ç»Ÿï¼š
1. çŸ­æœŸè®°å¿†: æ»‘åŠ¨çª—å£ + Redis Checkpointer
2. é•¿å¯¹è¯æ‘˜è¦: è‡ªåŠ¨æ‘˜è¦è¶…é•¿å¯¹è¯
3. è¯­ä¹‰è®°å¿†: åŸºäºå‘é‡çš„è·¨ä¼šè¯è®°å¿†
"""
import json
import logging
from typing import Dict, Any, List, Optional, AsyncGenerator
from datetime import datetime

from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage

from ..models.state import AgentState, ToolCall, StreamEvent
from ..tools.registry import tool_registry
from ..config import (
    OPENAI_API_KEY, OPENAI_MODEL, OPENAI_BASE_URL, MAX_ITERATIONS,
    SLIDING_WINDOW_SIZE, ENABLE_CONVERSATION_SUMMARY, ENABLE_SEMANTIC_MEMORY,
    ENABLE_CHECKPOINTER
)
from ..memory.sliding_window import SmartSlidingWindow
from ..memory.checkpointer import get_checkpointer
from ..memory.summary import get_summary_manager
from ..memory.semantic_memory import get_semantic_memory_service
from ..utils.circuit_breaker import get_breaker_manager, TOOL_ALTERNATIVES

logger = logging.getLogger(__name__)

# System prompt with memory context placeholder
SYSTEM_PROMPT = """ä½ æ˜¯ ResearchGO çš„ AI ç ”ç©¶åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è¿›è¡Œå­¦æœ¯ç ”ç©¶ã€‚

ä½ çš„èƒ½åŠ›ï¼š
1. æœç´¢å­¦æœ¯æ–‡çŒ®ï¼ˆOpenAlex æ•°æ®åº“ï¼ŒåŒ…å«æ•°äº¿ç¯‡è®ºæ–‡ï¼‰
2. ç®¡ç†ç”¨æˆ·çš„è®ºæ–‡åº“ï¼ˆä¸Šä¼ ã€æœç´¢ã€æŸ¥çœ‹ï¼‰
3. è¯­ä¹‰æœç´¢ï¼ˆåŸºäºå†…å®¹çš„æ™ºèƒ½æ£€ç´¢ï¼‰
4. è®ºæ–‡é—®ç­”ï¼ˆåŸºäºè®ºæ–‡å†…å®¹å›ç­”é—®é¢˜ï¼‰
5. è®ºæ–‡åˆ†æï¼ˆç”Ÿæˆåˆ†ææŠ¥å‘Šã€æ€ç»´å¯¼å›¾ï¼‰
6. è®ºæ–‡å¯¹æ¯”ï¼ˆå¯¹æ¯”å¤šç¯‡è®ºæ–‡çš„å¼‚åŒï¼‰

å·¥ä½œåŸåˆ™ï¼š
1. ä¼˜å…ˆç†è§£ç”¨æˆ·æ„å›¾ï¼Œé€‰æ‹©æœ€åˆé€‚çš„å·¥å…·
2. å¦‚æœéœ€è¦å¤šæ­¥æ“ä½œï¼ŒæŒ‰é¡ºåºæ‰§è¡Œ
3. å·¥å…·è¿”å›ç»“æœåï¼Œç”¨è‡ªç„¶è¯­è¨€æ€»ç»“ç»™ç”¨æˆ·
4. **æ™ºèƒ½é™çº§åŸåˆ™**ï¼ˆé‡è¦ï¼‰ï¼š
   - å¦‚æœå·¥å…·è°ƒç”¨å¤±è´¥æˆ–è¿”å›é™çº§é€šçŸ¥ï¼Œ**ä¸è¦ç›´æ¥å‘Šè¯‰ç”¨æˆ·"è¯·ç¨åé‡è¯•"**
   - é¦–å…ˆæŸ¥çœ‹é™çº§é€šçŸ¥ä¸­çš„æ›¿ä»£æ–¹æ¡ˆå»ºè®®
   - å°è¯•ä½¿ç”¨æ›¿ä»£å·¥å…·å®Œæˆç”¨æˆ·éœ€æ±‚
   - å¦‚æœæ²¡æœ‰åˆé€‚çš„æ›¿ä»£å·¥å…·ï¼Œå°è¯•åŸºäºä½ çš„çŸ¥è¯†ç›´æ¥å›ç­”
   - åªæœ‰åœ¨å®Œå…¨æ— æ³•å¸®åŠ©æ—¶ï¼Œæ‰å‘ŠçŸ¥ç”¨æˆ·å¹¶æä¾›æ›¿ä»£å»ºè®®
5. å›ç­”è¦ç®€æ´æ¸…æ™°ï¼Œé‡ç‚¹çªå‡º

ä½ å¯ä»¥ä½¿ç”¨çš„å·¥å…·ï¼š
{tool_descriptions}
{memory_context}
{degraded_tools_notice}
è®°ä½ï¼šä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ï¼Œå³ä½¿æŸäº›å·¥å…·æš‚æ—¶ä¸å¯ç”¨ï¼Œä¹Ÿè¦æƒ³åŠæ³•å¸®åŠ©ç”¨æˆ·å®Œæˆä»»åŠ¡ã€‚"""


# é™çº§å·¥å…·é€šçŸ¥æ¨¡æ¿
DEGRADED_TOOLS_NOTICE = """
âš ï¸ **å½“å‰ä¸å¯ç”¨çš„å·¥å…·**ï¼š{tools}

è¿™äº›å·¥å…·å› ä¸ºæœåŠ¡é—®é¢˜æš‚æ—¶ç†”æ–­ã€‚å½“ç”¨æˆ·è¯·æ±‚ç›¸å…³åŠŸèƒ½æ—¶ï¼š
1. ä¼˜å…ˆä½¿ç”¨æ›¿ä»£å·¥å…·
2. æˆ–åŸºäºä½ çš„çŸ¥è¯†å›ç­”
3. ä¸è¦ç®€å•åœ°è¯´"è¯·ç¨åé‡è¯•"
"""


class ResearchAgent:
    """
    ResearchGO Agent åŸºäº LangGraph
    
    é›†æˆä¸‰å±‚è®°å¿†æ¶æ„ï¼š
    - çŸ­æœŸè®°å¿†: æ»‘åŠ¨çª—å£é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
    - é•¿å¯¹è¯æ‘˜è¦: è¶…é•¿å¯¹è¯è‡ªåŠ¨ç”Ÿæˆæ‘˜è¦
    - è¯­ä¹‰è®°å¿†: è·¨ä¼šè¯çš„ç”¨æˆ·ç”»åƒå’Œåå¥½
    """
    
    def __init__(self):
        # åˆå§‹åŒ– LLMï¼ˆå¯ç”¨æµå¼è¾“å‡ºï¼‰
        llm_kwargs = {
            "model": OPENAI_MODEL,
            "temperature": 0.7,
            "api_key": OPENAI_API_KEY,
            "streaming": True,  # å¯ç”¨æµå¼è¾“å‡º
        }
        if OPENAI_BASE_URL:
            llm_kwargs["base_url"] = OPENAI_BASE_URL
        
        self.llm = ChatOpenAI(**llm_kwargs)
        
        # ç»‘å®šå·¥å…·åˆ° LLM
        self.tools = tool_registry.get_openai_functions()
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
        # åˆå§‹åŒ–è®°å¿†ç»„ä»¶
        self.sliding_window = SmartSlidingWindow(window_size=SLIDING_WINDOW_SIZE)
        self.checkpointer = get_checkpointer() if ENABLE_CHECKPOINTER else None
        
        # æ„å»ºå›¾
        self.graph = self._build_graph()
        
        # ç¼–è¯‘å›¾ï¼ˆå¸¦/ä¸å¸¦ checkpointerï¼‰
        if self.checkpointer:
            self.app = self.graph.compile(checkpointer=self.checkpointer)
            logger.info("Agent compiled with Redis Checkpointer")
        else:
            self.app = self.graph.compile()
            logger.info("Agent compiled without Checkpointer")
    
    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph çŠ¶æ€å›¾"""
        graph = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("reason", self._reason_node)
        graph.add_node("execute_tools", self._execute_tools_node)
        graph.add_node("respond", self._respond_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        graph.set_entry_point("reason")
        
        # æ·»åŠ æ¡ä»¶è¾¹
        graph.add_conditional_edges(
            "reason",
            self._should_continue,
            {
                "execute_tools": "execute_tools",
                "respond": "respond",
                "end": END
            }
        )
        
        # å·¥å…·æ‰§è¡Œåå›åˆ°æ¨ç†èŠ‚ç‚¹
        graph.add_edge("execute_tools", "reason")
        
        # å“åº”èŠ‚ç‚¹ç»“æŸ
        graph.add_edge("respond", END)
        
        return graph
    
    async def _prepare_context(
        self,
        messages: List[Dict[str, Any]],
        user_id: Optional[str],
        user_input: str,
        conversation_id: Optional[str],
        token: Optional[str]
    ) -> tuple[List[Dict[str, Any]], str, str]:
        """
        å‡†å¤‡ä¸Šä¸‹æ–‡ï¼šåº”ç”¨ä¸‰å±‚è®°å¿†ç³»ç»Ÿ
        
        Returns:
            (å¤„ç†åçš„æ¶ˆæ¯, å¯¹è¯æ‘˜è¦, ç”¨æˆ·è®°å¿†ä¸Šä¸‹æ–‡)
        """
        summary = ""
        memory_context = ""
        processed_messages = messages
        
        # 1. é•¿å¯¹è¯æ‘˜è¦
        if ENABLE_CONVERSATION_SUMMARY and conversation_id:
            try:
                summary_manager = get_summary_manager()
                summary_result = await summary_manager.process(
                    messages=messages,
                    conversation_id=conversation_id,
                    window_size=SLIDING_WINDOW_SIZE
                )
                
                if summary_result.summary:
                    summary = summary_result.summary
                    processed_messages = summary_result.window_messages
                    logger.info(
                        f"Applied summary: {summary_result.original_count} -> "
                        f"{len(processed_messages)} messages (summarized: {summary_result.summarized_count})"
                    )
            except Exception as e:
                logger.warning(f"Summary processing failed: {e}")
        
        # 2. æ»‘åŠ¨çª—å£ï¼ˆåœ¨æ‘˜è¦ä¹‹ååº”ç”¨ï¼Œè¿›ä¸€æ­¥æ§åˆ¶ Tokenï¼‰
        processed_messages, window_stats = self.sliding_window.apply(
            processed_messages,
            strategy="hybrid"
        )
        
        if window_stats.messages_dropped > 0:
            logger.info(f"Sliding window dropped {window_stats.messages_dropped} messages")
        
        # 3. è¯­ä¹‰è®°å¿†ï¼ˆè·å–ç”¨æˆ·ç›¸å…³ä¸Šä¸‹æ–‡ï¼‰
        if ENABLE_SEMANTIC_MEMORY and user_id and token:
            try:
                semantic_memory = get_semantic_memory_service()
                memory_context = await semantic_memory.get_user_context(
                    user_id=user_id,
                    current_query=user_input,
                    token=token
                )
                
                if memory_context:
                    logger.info(f"Retrieved semantic memory context for user {user_id}")
            except Exception as e:
                logger.warning(f"Semantic memory retrieval failed: {e}")
        
        return processed_messages, summary, memory_context
    
    async def _reason_node(self, state: AgentState) -> Dict[str, Any]:
        """æ¨ç†èŠ‚ç‚¹ï¼šåˆ†æç”¨æˆ·æ„å›¾ï¼Œå†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·"""
        logger.info(f"Reason node - iteration: {state.get('iteration', 0)}")
        
        # è·å–çŠ¶æ€ä¿¡æ¯
        raw_messages = state.get("messages", [])
        user_id = state.get("user_id")
        user_input = state.get("user_input", "")
        conversation_id = state.get("conversation_id")
        token = state.get("token")
        
        logger.debug(f"Reason node: user_input='{user_input[:50] if user_input else ''}...', messages={len(raw_messages)}")
        
        # åº”ç”¨ä¸‰å±‚è®°å¿†ç³»ç»Ÿ
        processed_messages, summary, memory_context = await self._prepare_context(
            messages=raw_messages,
            user_id=user_id,
            user_input=user_input,
            conversation_id=conversation_id,
            token=token
        )
        
        # æ„å»º memory context éƒ¨åˆ†
        memory_section = ""
        if summary or memory_context:
            memory_section = "\n\n--- ä¸Šä¸‹æ–‡ä¿¡æ¯ ---"
            if summary:
                memory_section += f"\n[å¯¹è¯æ‘˜è¦]: {summary}"
            if memory_context:
                memory_section += f"\n[ç”¨æˆ·èƒŒæ™¯]:\n{memory_context}"
            memory_section += "\n--- ä¸Šä¸‹æ–‡ç»“æŸ ---\n"
        
        # è·å–å½“å‰è¢«ç†”æ–­çš„å·¥å…·
        degraded_tools_notice = ""
        breaker_manager = get_breaker_manager()
        degraded_tools = breaker_manager.get_degraded_tools()
        if degraded_tools:
            # æ„å»ºè¯¦ç»†çš„é™çº§å·¥å…·é€šçŸ¥ï¼ŒåŒ…å«æ›¿ä»£æ–¹æ¡ˆ
            degraded_info_list = []
            for tool_name in degraded_tools:
                alt_info = TOOL_ALTERNATIVES.get(tool_name, {})
                alternatives = alt_info.get("alternatives", [])
                hint = alt_info.get("hint", "")
                if alternatives:
                    degraded_info_list.append(f"- {tool_name} â†’ æ›¿ä»£æ–¹æ¡ˆ: {', '.join(alternatives)}")
                else:
                    degraded_info_list.append(f"- {tool_name} â†’ æ— æ›¿ä»£å·¥å…·ï¼Œéœ€ç›´æ¥å›ç­”")
            
            degraded_tools_notice = DEGRADED_TOOLS_NOTICE.format(
                tools="\n".join(degraded_info_list)
            )
            logger.warning(f"å½“å‰è¢«ç†”æ–­çš„å·¥å…·: {degraded_tools}")
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            SystemMessage(content=SYSTEM_PROMPT.format(
                tool_descriptions=tool_registry.get_tool_descriptions(),
                memory_context=memory_section,
                degraded_tools_notice=degraded_tools_notice
            ))
        ]
        
        # æ·»åŠ å¤„ç†åçš„å†å²æ¶ˆæ¯
        for msg in processed_messages:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                # æ£€æŸ¥æ˜¯å¦æœ‰ tool_calls
                if msg.get("tool_calls"):
                    # åˆ›å»ºå¸¦æœ‰ tool_calls çš„ AIMessage
                    ai_msg = AIMessage(
                        content=msg.get("content", ""),
                        tool_calls=msg["tool_calls"]
                    )
                    messages.append(ai_msg)
                else:
                    messages.append(AIMessage(content=msg.get("content", "")))
            elif msg["role"] == "tool":
                messages.append(ToolMessage(
                    content=msg["content"],
                    tool_call_id=msg.get("tool_call_id", "")
                ))
        
        logger.debug(f"Calling LLM with {len(messages)} messages")
        
        # è°ƒç”¨ LLM
        response = await self.llm_with_tools.ainvoke(messages)
        
        # å¤„ç†å“åº”
        tool_calls = []
        thoughts = state.get("thoughts", [])
        
        if response.tool_calls:
            # LLM å†³å®šè°ƒç”¨å·¥å…·
            for tc in response.tool_calls:
                tool_calls.append(ToolCall(
                    id=tc["id"],
                    name=tc["name"],
                    arguments=tc["args"]
                ))
                thoughts.append(f"ğŸ”§ å‡†å¤‡è°ƒç”¨å·¥å…·: {tc['name']}")
            
            return {
                "messages": [{"role": "assistant", "content": response.content or "", "tool_calls": response.tool_calls}],
                "tool_calls": tool_calls,
                "thoughts": thoughts,
                "should_continue": True,
                "iteration": state.get("iteration", 0) + 1
            }
        else:
            # LLM ç›´æ¥å›ç­”
            return {
                "messages": [{"role": "assistant", "content": response.content}],
                "final_answer": response.content,
                "should_continue": False,
                "iteration": state.get("iteration", 0) + 1
            }
    
    async def _execute_tools_node(self, state: AgentState) -> Dict[str, Any]:
        """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹"""
        logger.info("Execute tools node")
        
        tool_calls = state.get("tool_calls", [])
        thoughts = state.get("thoughts", [])
        new_messages = []
        
        for tc in tool_calls:
            tool = tool_registry.get(tc.name)
            if not tool:
                error_msg = f"å·¥å…· {tc.name} ä¸å­˜åœ¨"
                thoughts.append(f"âŒ {error_msg}")
                new_messages.append({
                    "role": "tool",
                    "content": json.dumps({"error": error_msg}),
                    "tool_call_id": tc.id
                })
                continue
            
            # æ‰§è¡Œå·¥å…·
            thoughts.append(f"âš™ï¸ æ­£åœ¨æ‰§è¡Œ: {tc.name}")
            
            # æ·»åŠ  tokenï¼ˆå¦‚æœçŠ¶æ€ä¸­æœ‰çš„è¯ï¼‰
            kwargs = tc.arguments.copy()
            if state.get("token"):
                kwargs["token"] = state["token"]
            
            result = await tool(**kwargs)
            
            if result.success:
                thoughts.append(f"âœ… {tc.name} æ‰§è¡ŒæˆåŠŸ")
                new_messages.append({
                    "role": "tool",
                    "content": json.dumps(result.data, ensure_ascii=False),
                    "tool_call_id": tc.id
                })
            elif result.is_degraded:
                # æ™ºèƒ½é™çº§å“åº”ï¼šå·¥å…·è¢«ç†”æ–­
                thoughts.append(f"âš ï¸ {tc.name} æœåŠ¡ç†”æ–­ï¼Œéœ€è¦æ™ºèƒ½é™çº§")
                
                # è·å–æ›¿ä»£æ–¹æ¡ˆä¿¡æ¯
                alt_info = TOOL_ALTERNATIVES.get(tc.name, {})
                alternatives = alt_info.get("alternatives", [])
                hint = alt_info.get("hint", "")
                
                # æ„å»ºç»™ LLM çš„é™çº§æŒ‡å¯¼
                degraded_guidance = {
                    "status": "degraded",
                    "tool": tc.name,
                    "message": "è¯¥å·¥å…·æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·é‡‡å–æ™ºèƒ½é™çº§ç­–ç•¥",
                    "alternatives": alternatives,
                    "hint": hint,
                    "instruction": "è¯·æ ¹æ®ä»¥ä¸Šæ›¿ä»£æ–¹æ¡ˆå°è¯•å…¶ä»–æ–¹æ³•å¸®åŠ©ç”¨æˆ·ï¼Œä¸è¦ç›´æ¥å‘Šè¯‰ç”¨æˆ·'è¯·ç¨åé‡è¯•'"
                }
                
                new_messages.append({
                    "role": "tool",
                    "content": json.dumps(degraded_guidance, ensure_ascii=False),
                    "tool_call_id": tc.id
                })
                
                logger.warning(f"å·¥å…· {tc.name} è¿”å›é™çº§å“åº”ï¼Œæ›¿ä»£æ–¹æ¡ˆ: {alternatives}")
            else:
                thoughts.append(f"âŒ {tc.name} æ‰§è¡Œå¤±è´¥: {result.error}")
                new_messages.append({
                    "role": "tool",
                    "content": json.dumps({"error": result.error}),
                    "tool_call_id": tc.id
                })
            
            # æ›´æ–°å·¥å…·è°ƒç”¨ç»“æœ
            tc.result = result.data if result.success else None
            tc.error = result.error if not result.success else None
            tc.duration_ms = result.duration_ms
        
        return {
            "messages": new_messages,
            "thoughts": thoughts,
            "tool_calls": []  # æ¸…ç©ºå·²æ‰§è¡Œçš„å·¥å…·è°ƒç”¨
        }
    
    async def _respond_node(self, state: AgentState) -> Dict[str, Any]:
        """å“åº”èŠ‚ç‚¹ï¼šç”Ÿæˆæœ€ç»ˆå›ç­”"""
        logger.info("Respond node")
        return {
            "final_answer": state.get("final_answer", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚")
        }
    
    def _should_continue(self, state: AgentState) -> str:
        """å†³å®šä¸‹ä¸€æ­¥èµ°å‘"""
        iteration = state.get("iteration", 0)
        
        # æ£€æŸ¥è¿­ä»£æ¬¡æ•°é™åˆ¶
        if iteration >= MAX_ITERATIONS:
            logger.warning(f"Max iterations ({MAX_ITERATIONS}) reached")
            return "respond"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·éœ€è¦æ‰§è¡Œ
        if state.get("tool_calls"):
            return "execute_tools"
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­
        if state.get("should_continue", False):
            return "execute_tools"
        
        # æœ‰æœ€ç»ˆç­”æ¡ˆåˆ™ç»“æŸ
        if state.get("final_answer"):
            return "end"
        
        return "respond"
    
    async def _post_process_memories(
        self,
        messages: List[Dict[str, Any]],
        user_id: Optional[str],
        token: Optional[str]
    ):
        """
        åå¤„ç†ï¼šä»å¯¹è¯ä¸­æå–å¹¶å­˜å‚¨è¯­ä¹‰è®°å¿†
        
        åœ¨å¯¹è¯ç»“æŸåå¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡å“åº”
        """
        if not ENABLE_SEMANTIC_MEMORY or not user_id or not token:
            return
        
        try:
            semantic_memory = get_semantic_memory_service()
            stored_count = await semantic_memory.process_and_store(
                messages=messages,
                user_id=user_id,
                token=token
            )
            
            if stored_count > 0:
                logger.info(f"Post-process: stored {stored_count} memories for user {user_id}")
        except Exception as e:
            logger.warning(f"Post-process memory storage failed: {e}")
    
    async def run(
        self,
        user_input: str,
        user_id: Optional[str] = None,
        token: Optional[str] = None,
        conversation_history: Optional[List[dict]] = None,
        conversation_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """è¿è¡Œ Agentï¼ˆéæµå¼ï¼‰"""
        initial_state = {
            "messages": conversation_history or [],
            "user_input": user_input,
            "user_id": user_id,
            "token": token,
            "conversation_id": conversation_id,
            "tool_calls": [],
            "iteration": 0,
            "should_continue": True,
            "final_answer": None,
            "error": None,
            "thoughts": []
        }
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        initial_state["messages"].append({"role": "user", "content": user_input})
        
        # æ„å»ºé…ç½®ï¼ˆç”¨äº checkpointerï¼‰
        config = {}
        if self.checkpointer and conversation_id:
            config = {"configurable": {"thread_id": f"conv_{conversation_id}"}}
        
        # è¿è¡Œå›¾
        final_state = await self.app.ainvoke(initial_state, config=config)
        
        # åå¤„ç†ï¼šæå–è¯­ä¹‰è®°å¿†ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡ï¼‰
        await self._post_process_memories(
            messages=final_state.get("messages", []),
            user_id=user_id,
            token=token
        )
        
        return {
            "answer": final_state.get("final_answer", ""),
            "thoughts": final_state.get("thoughts", []),
            "tool_calls": final_state.get("tool_calls", [])
        }
    
    async def run_stream(
        self,
        user_input: str,
        user_id: Optional[str] = None,
        token: Optional[str] = None,
        conversation_history: Optional[List[dict]] = None,
        conversation_id: Optional[str] = None
    ) -> AsyncGenerator[StreamEvent, None]:
        """è¿è¡Œ Agentï¼ˆæµå¼ï¼‰"""
        initial_state = {
            "messages": conversation_history or [],
            "user_input": user_input,
            "user_id": user_id,
            "token": token,
            "conversation_id": conversation_id,
            "tool_calls": [],
            "iteration": 0,
            "should_continue": True,
            "final_answer": None,
            "error": None,
            "thoughts": []
        }
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        initial_state["messages"].append({"role": "user", "content": user_input})
        
        # æ„å»ºé…ç½®ï¼ˆç”¨äº checkpointerï¼‰
        config = {}
        if self.checkpointer and conversation_id:
            config = {"configurable": {"thread_id": f"conv_{conversation_id}"}}
        
        # æµå¼è¿è¡Œ - ä½¿ç”¨ astream_events è·å– token çº§åˆ«æµå¼è¾“å‡º
        last_thoughts_count = 0
        final_messages = []
        final_answer_chunks = []
        
        async for event in self.app.astream_events(initial_state, config=config, version="v2"):
            event_type = event.get("event")
            
            # LLM token æµå¼è¾“å‡º
            if event_type == "on_chat_model_stream":
                chunk = event.get("data", {}).get("chunk")
                if chunk and hasattr(chunk, "content") and chunk.content:
                    final_answer_chunks.append(chunk.content)
                    yield StreamEvent(event="token", data=chunk.content)
            
            # èŠ‚ç‚¹å¼€å§‹
            elif event_type == "on_chain_start":
                node_name = event.get("name", "")
                if node_name in ["reason", "execute_tools", "respond"]:
                    yield StreamEvent(event="node_start", data=node_name)
            
            # èŠ‚ç‚¹ç»“æŸ
            elif event_type == "on_chain_end":
                node_name = event.get("name", "")
                output = event.get("data", {}).get("output", {})
                
                if isinstance(output, dict):
                    # æ”¶é›†æ¶ˆæ¯ç”¨äºåå¤„ç†
                    if output.get("messages"):
                        final_messages.extend(output["messages"])
                    
                    # å‘é€æ€è€ƒè¿‡ç¨‹
                    thoughts = output.get("thoughts", [])
                    if len(thoughts) > last_thoughts_count:
                        for thought in thoughts[last_thoughts_count:]:
                            yield StreamEvent(event="thinking", data=thought)
                        last_thoughts_count = len(thoughts)
                    
                    # å‘é€å·¥å…·è°ƒç”¨ä¿¡æ¯
                    if output.get("tool_calls"):
                        for tc in output["tool_calls"]:
                            yield StreamEvent(
                                event="tool_call",
                                data={
                                    "name": tc.name,
                                    "arguments": tc.arguments
                                }
                            )
                    
                    # å‘é€å·¥å…·æ‰§è¡Œç»“æœ
                    messages = output.get("messages", [])
                    for msg in messages:
                        if msg.get("role") == "tool":
                            try:
                                tool_data = json.loads(msg.get("content", "{}"))
                                if tool_data.get("results") and isinstance(tool_data["results"], list):
                                    if tool_data["results"] and "title" in tool_data["results"][0]:
                                        yield StreamEvent(
                                            event="papers",
                                            data={
                                                "query": tool_data.get("query", ""),
                                                "total": tool_data.get("total_count", len(tool_data["results"])),
                                                "papers": tool_data["results"]
                                            }
                                        )
                            except:
                                pass
                    
                    # å‘é€æœ€ç»ˆç­”æ¡ˆå®Œæˆä¿¡å·
                    if output.get("final_answer"):
                        # å¦‚æœä¹‹å‰æ²¡æœ‰æµå¼è¾“å‡º tokenï¼Œå‘é€å®Œæ•´ç­”æ¡ˆ
                        if not final_answer_chunks:
                            yield StreamEvent(event="answer", data=output["final_answer"])
                        else:
                            yield StreamEvent(event="answer_end", data=None)
        
        # åå¤„ç†ï¼šæå–è¯­ä¹‰è®°å¿†
        await self._post_process_memories(
            messages=initial_state["messages"] + final_messages,
            user_id=user_id,
            token=token
        )
        
        yield StreamEvent(event="done", data=None)


# å…¨å±€ Agent å®ä¾‹
_agent_instance: Optional[ResearchAgent] = None


def get_agent() -> ResearchAgent:
    """è·å– Agent å•ä¾‹"""
    global _agent_instance
    if _agent_instance is None:
        _agent_instance = ResearchAgent()
    return _agent_instance
